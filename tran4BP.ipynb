{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sample numbers: 4000 4000 4000\n",
      "Train set shape: (12000, 8)\n",
      "Label set shape: (12000,)\n",
      "Unique labels distribution: (array([0., 1., 2.]), array([4000, 4000, 4000], dtype=int64))\n"
     ]
    }
   ],
   "source": [
    "# Licensed under the Apache License, Version 2.0 (the \"License\");\n",
    "# you may not use this file except in compliance with the License.\n",
    "# You may obtain a copy of the License at\n",
    "#\n",
    "#     http://www.apache.org/licenses/LICENSE-2.0\n",
    "#\n",
    "# Unless required by applicable law or agreed to in writing, software\n",
    "# distributed under the License is distributed on an \"AS IS\" BASIS,\n",
    "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
    "# See the License for the specific language governing permissions and\n",
    "# limitations under the License.\n",
    "\n",
    "# Author: Weijie Zou\n",
    "\n",
    "import os\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from utils.data_vis import plot_img_and_mask, plot_imgs, plot_mask\n",
    "\n",
    "# Path to the images and masks\n",
    "imgs_path = r'F:\\Workspace\\Projects\\气象局技能大赛\\基于机器学习的晴空回波识别\\Data_adjusted_psahu_400\\imgs'\n",
    "masks_path = r'F:\\Workspace\\Projects\\气象局技能大赛\\基于机器学习的晴空回波识别\\Data_adjusted_psahu_400\\masks'\n",
    "\n",
    "# List of files (without extensions) in the masks path\n",
    "files_sv = os.listdir(masks_path)\n",
    "files_sv = [f.split('.')[0] for f in files_sv]\n",
    "\n",
    "# Initialize lists to store features for each class\n",
    "r_0, v_0, w_0, ldr_0, text_r_0, text_v_0, text_w_0, height_0 = [], [], [], [], [], [], [], []\n",
    "r_1, v_1, w_1, ldr_1, text_r_1, text_v_1, text_w_1, height_1 = [], [], [], [], [], [], [], []\n",
    "r_2, v_2, w_2, ldr_2, text_r_2, text_v_2, text_w_2, height_2 = [], [], [], [], [], [], [], []\n",
    "\n",
    "# Loop through the files and process each image and corresponding mask\n",
    "for f in files_sv[:10]:  # Limiting to the first 10 files for testing\n",
    "    # Load the image and mask\n",
    "    img = np.load(os.path.join(imgs_path, f + '.npy'))\n",
    "    r, v, w, ldr = img[:, :, 0], img[:, :, 1], img[:, :, 2], img[:, :, 3]\n",
    "    mask = np.load(os.path.join(masks_path, f + '.npy'))\n",
    "\n",
    "    # Create an echo mask based on conditions\n",
    "    echo_mask = np.full(mask.shape, np.nan)\n",
    "    echo_mask[r >= -50] = 1  # Clear-air echo\n",
    "    echo_mask[v >= -15] = 1   # Clear-air echo\n",
    "    echo_mask[w >= 0] = 1     # Clear-air echo\n",
    "    echo_mask[mask == 1] += 1  # Meteorological echo\n",
    "    echo_mask[:, 150:][echo_mask[:, 150:] == 1] = 2  #Meteorological echo\n",
    "    mask[echo_mask == 2] = 1  # Adjust mask for Meteorological echo\n",
    "\n",
    "    # Compute texture features for the image\n",
    "    Text_r_m, Text_v_m, Text_w_m = np.zeros_like(r), np.zeros_like(v), np.zeros_like(w)\n",
    "    for i in range(1, r.shape[0]):\n",
    "        for j in range(1, r.shape[1]):\n",
    "            Text_r_m[i, j] = np.abs(r[i, j] - r[i - 1, j])**2\n",
    "            Text_v_m[i, j] = np.abs(v[i, j] - v[i - 1, j])**2\n",
    "            Text_w_m[i, j] = np.abs(w[i, j] - w[i - 1, j])**2\n",
    "\n",
    "    # Smooth texture features to create a more uniform representation\n",
    "    Text_r, Text_v, Text_w, H = np.zeros_like(r), np.zeros_like(v), np.zeros_like(w), np.zeros_like(r)\n",
    "    Height = np.arange(r.shape[1]) * 0.03  # Height based on range\n",
    "    for i in range(r.shape[0]):\n",
    "        for j in range(r.shape[1]):\n",
    "            i_min, i_max = max(i - 2, 0), min(i + 2, r.shape[0] - 1)\n",
    "            j_min, j_max = max(j - 2, 0), min(j + 2, r.shape[1] - 1)\n",
    "            Text_r[i, j] = np.sum(Text_r_m[i_min:i_max + 1, j_min:j_max + 1]) / ((i_max - i_min + 1) * (j_max - j_min + 1))\n",
    "            Text_v[i, j] = np.sum(Text_v_m[i_min:i_max + 1, j_min:j_max + 1]) / ((i_max - i_min + 1) * (j_max - j_min + 1))\n",
    "            Text_w[i, j] = np.sum(Text_w_m[i_min:i_max + 1, j_min:j_max + 1]) / ((i_max - i_min + 1) * (j_max - j_min + 1))\n",
    "            H[i, j] = Height[j]\n",
    "\n",
    "    # Replace NaN values in the mask\n",
    "    echomask = echo_mask\n",
    "    echomask[np.isnan(echomask)] = 0\n",
    "\n",
    "    # Append features for each echo class\n",
    "    r_0.append(r[echomask == 0].flatten())\n",
    "    v_0.append(v[echomask == 0].flatten())\n",
    "    w_0.append(w[echomask == 0].flatten())\n",
    "    ldr_0.append(ldr[echomask == 0].flatten())\n",
    "    text_r_0.append(Text_r[echomask == 0].flatten())\n",
    "    text_v_0.append(Text_v[echomask == 0].flatten())\n",
    "    text_w_0.append(Text_w[echomask == 0].flatten())\n",
    "    height_0.append(H[echomask == 0].flatten())\n",
    "\n",
    "    r_1.append(r[echomask == 1].flatten())\n",
    "    v_1.append(v[echomask == 1].flatten())\n",
    "    w_1.append(w[echomask == 1].flatten())\n",
    "    ldr_1.append(ldr[echomask == 1].flatten())\n",
    "    text_r_1.append(Text_r[echomask == 1].flatten())\n",
    "    text_v_1.append(Text_v[echomask == 1].flatten())\n",
    "    text_w_1.append(Text_w[echomask == 1].flatten())\n",
    "    height_1.append(H[echomask == 1].flatten())\n",
    "\n",
    "    r_2.append(r[echomask == 2].flatten())\n",
    "    v_2.append(v[echomask == 2].flatten())\n",
    "    w_2.append(w[echomask == 2].flatten())\n",
    "    ldr_2.append(ldr[echomask == 2].flatten())\n",
    "    text_r_2.append(Text_r[echomask == 2].flatten())\n",
    "    text_v_2.append(Text_v[echomask == 2].flatten())\n",
    "    text_w_2.append(Text_w[echomask == 2].flatten())\n",
    "    height_2.append(H[echomask == 2].flatten())\n",
    "\n",
    "# Concatenate all features for each echo class\n",
    "r_0, v_0, w_0, ldr_0, text_r_0, text_v_0, text_w_0, height_0 = map(np.concatenate, [r_0, v_0, w_0, ldr_0, text_r_0, text_v_0, text_w_0, height_0])\n",
    "r_1, v_1, w_1, ldr_1, text_r_1, text_v_1, text_w_1, height_1 = map(np.concatenate, [r_1, v_1, w_1, ldr_1, text_r_1, text_v_1, text_w_1, height_1])\n",
    "r_2, v_2, w_2, ldr_2, text_r_2, text_v_2, text_w_2, height_2 = map(np.concatenate, [r_2, v_2, w_2, ldr_2, text_r_2, text_v_2, text_w_2, height_2])\n",
    "\n",
    "# Create training sets for each class by stacking features\n",
    "train_0 = np.stack([r_0, v_0, w_0, ldr_0, text_r_0, text_v_0, text_w_0, height_0], axis=-1).reshape(-1, 8)\n",
    "train_1 = np.stack([r_1, v_1, w_1, ldr_1, text_r_1, text_v_1, text_w_1, height_1], axis=-1).reshape(-1, 8)\n",
    "train_2 = np.stack([r_2, v_2, w_2, ldr_2, text_r_2, text_v_2, text_w_2, height_2], axis=-1).reshape(-1, 8)\n",
    "\n",
    "# Define sample size\n",
    "sample_size = 4000\n",
    "\n",
    "# Randomly select a sample of size `sample_size` from each class\n",
    "sample_size_0 = min(sample_size, train_0.shape[0])\n",
    "sample_size_1 = min(sample_size, train_1.shape[0])\n",
    "sample_size_2 = min(sample_size, train_2.shape[0])\n",
    "\n",
    "print(\"Sample numbers:\", sample_size_0, sample_size_1, sample_size_2)\n",
    "\n",
    "# Randomly select samples\n",
    "indices_0 = np.random.choice(train_0.shape[0], sample_size_0, replace=False)\n",
    "indices_1 = np.random.choice(train_1.shape[0], sample_size_1, replace=False)\n",
    "indices_2 = np.random.choice(train_2.shape[0], sample_size_2, replace=False)\n",
    "\n",
    "train_0_sampled = train_0[indices_0]\n",
    "train_1_sampled = train_1[indices_1]\n",
    "train_2_sampled = train_2[indices_2]\n",
    "\n",
    "# Generate corresponding labels for each class\n",
    "labels_0 = np.zeros(sample_size_0)\n",
    "labels_1 = np.ones(sample_size_1)\n",
    "labels_2 = np.full(sample_size_2, 2)\n",
    "\n",
    "# Combine training data and labels\n",
    "train_set = np.concatenate([train_0_sampled, train_1_sampled, train_2_sampled], axis=0)\n",
    "train_set[np.isnan(train_set)] = -999  # Handle NaN values by replacing with -999\n",
    "labels_set = np.concatenate([labels_0, labels_1, labels_2], axis=0)\n",
    "\n",
    "# Shuffle the training set and labels\n",
    "indices = np.arange(train_set.shape[0])\n",
    "np.random.shuffle(indices)\n",
    "train_set = train_set[indices]\n",
    "labels_set = labels_set[indices]\n",
    "\n",
    "# Output shapes\n",
    "print(\"Train set shape:\", train_set.shape)\n",
    "print(\"Label set shape:\", labels_set.shape)\n",
    "print(\"Unique labels distribution:\", np.unique(labels_set, return_counts=True))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['model_BP.joblib']"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Train model\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "\n",
    "X_train, y_train = train_set, labels_set\n",
    "\n",
    "\n",
    "clf = MLPClassifier(hidden_layer_sizes=(20, 50, 50, 20), max_iter=10000)\n",
    "clf.fit(X_train, y_train)\n",
    "\n",
    "# Save model\n",
    "from joblib import dump, load\n",
    "dump(clf, 'model_BP.joblib')\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
